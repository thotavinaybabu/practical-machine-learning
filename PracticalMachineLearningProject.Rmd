---
title: "PracticalMachineLearningProject"
author: "vinay babu thota"
date: "08/10/2016"
output: html_document
---
<!-- rmarkdown v1 -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(repmis)
```

## Practical Machine Learning Project -Prediction 
### Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
The main goal of your project is to predict the manner in which they did the exercise.

The training data for this project are available at:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available at:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

We will use the "classe" variable in the training set for prediction.

We will  create a report describing how we built your model, how we used cross validation, what we think the expected out of sample error is, and why we made the choices we did.

## Exploratory Data Analysis

``````{r ReadCSV, echo=TRUE}
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))

```

We now delete columns (predictors) of the training and testing set that contain any missing values.Then remove the first seven predictors since these variables have little predicting power for the outcome classe.

```{r CleanData ,echo=TRUE}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
dim(trainData)
dim(testData)
```

The cleaned data sets trainData and testData both have 53 columns with the same first 52 variables and the last variable classe and problem_id respectively. trainData has 19622 rows while testData has 20 rows.

We will use the training dataset for building the model
## Data Splitting

In order to get out-of-sample errors, we split the cleaned training set trainData into a training set (train, 70%) for prediction and a validation set (valid 30%) to compute the out-of-sample errors.



```{r DataSplitting, echo=TRUE}
set.seed(1234) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```

## Prediction Algorithms

### Classification trees

In practice, k=5k=5 or k=10k=10 when doing k-fold cross validation. Here we consider 5-fold cross validation (default setting in trainControl function is 10) when implementing the algorithm to save a little computing time. Since data transformations may be less important in non-linear models like classification trees, we do not transform any variables.

```{r classificationTrees,echo=TRUE}
control <- trainControl(method = "cv", number = 3)
fit_rpart <- train(classe ~ ., data = train, method = "rpart",trControl = control)
print(fit_rpart, digits = 4)
fancyRpartPlot(fit_rpart$finalModel)
```

# Predicting outcomes using validation set and Showing  Prediction result

```{r prediction,echo=TRUE}
predict_rpart <- predict(fit_rpart, valid)
conf_rpart <- confusionMatrix(valid$classe, predict_rpart)
conf_rpart
accuracy_rpart <- conf_rpart$overall[1]
accuracy_rpart
```

From the confusion matrix, the accuracy rate is 0.489, and so the out-of-sample error rate is 0.355. Using classification tree does not predict the outcome classe very well.

## Random forests

Since classification tree method does not perform well, we try random forest method instead.

```{r randomForest,echo=TRUE}
fit_rf <- train(classe ~ ., data = train, method = "parRF", ntree = 250,
                   trControl = control)
print(fit_rf, digits = 4)
```

# Predicting outcomes using validation set and showing  prediction result

```{r randomForestPredict,echo=TRUE}
predict_rf <- predict(fit_rf, valid)

(conf_rf <- confusionMatrix(valid$classe, predict_rf))

(accuracy_rf <- conf_rf$overall[1])

```

For this dataset, random forest method is way better than classification tree method. The accuracy rate is 0.994

# Prediction on Testing Set
We now use random forests to predict the outcome variable classe for the testing set.

```{r predictTest,echo=TRUE}
(predict(fit_rf, testData))
```